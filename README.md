# GPTProjectTVSeries

## **Overview**
This project is to replicate and build upon the GPT-1 architecture, following the guidance of [Andrej Karpathy's YouTube series](https://www.youtube.com/user/karpathy). The goal is to develop and fine-tune a basic large language model (LLM) on a chosen dataset while balancing computational efficiency and output quality through iterative improvements.

This was done for the IS 640 Python Group Project

## **Project Objectives**
1. Understand and implement key components of transformer architecture:
   - Self-attention
   - Positional encoding
   - Embedding layers
2. Pre-process and tokenize text datasets.
3. Train a language model and evaluate its performance.
4. Generate coherent text and refine outputs iteratively.
5. Experiment with parameters to optimize the trade-off between model performance and computational efficiency.

---

## **Milestones**
The project consists of eight milestones, each focusing on incremental improvements to the model:
1. **Dataset Exploration and Preparation**:
   - Select, clean, and tokenize a dataset.
   - Justify dataset choice in the project report.
2. **Basic Model Usage (Bigram Language Model)**:
   - Train and evaluate a bigram model on the dataset.
3. **Self-Attention & Softmax Iteration**:
   - Integrate self-attention into the model.
4. **Multi-Head Attention**:
   - Add multi-head attention for richer representations.
5. **Feed Forward Layers**:
   - Implement feed-forward layers.
6. **Residual Connections**:
   - Include residual connections to enhance training stability.
7. **Layer Normalization**:
   - Add layer normalization for better gradient flow.
8. **Dropout**:
   - Integrate dropout to reduce overfitting.
