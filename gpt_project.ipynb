{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 5 GPT Project IS 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/suraj520/imdb-tv-series-data?select=musical_series.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1: Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description: \n",
    "This dataset contains information about TV series from IMDb, including details such as title, IMDb ID, release year, genre, cast, synopsis, rating, runtime, certificate, number of votes, and gross revenue. The data is scraped from the IMDb website using web scraping techniques and is organized into separate CSV files for each genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features:\n",
    "\n",
    "- Title: The title of the TV series.\n",
    "- IMDb ID: The unique identifier for the series on IMDb.\n",
    "- Release Year: The year in which the series was released.\n",
    "- Genre: The genre(s) of the series.\n",
    "- Cast: The main cast members of the series.\n",
    "- Synopsis: A brief summary or description of the series.\n",
    "- Rating: The average rating of the series on IMDb (scaled from 1 to 10).\n",
    "- Runtime: The duration of each episode or the total runtime of the series.\n",
    "- Certificate: The content rating or certificate assigned to the series (e.g., PG-13, TV-MA).\n",
    "- Number of Votes: The total number of votes or ratings received by the series.\n",
    "- Gross Revenue: The total gross revenue generated by the series (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "\n",
    "We aim to generate text using the GPT transformer model, focusing exclusively on the 'Synopsis' column of the TV series dataset. Our goal is to clean and preprocess the 'Synopsis' data by converting all text to lowercase and replacing non-alphanumeric characters (except dots) with spaces, and then utilize the GPT transformer to generate coherent and relevant text based on the cleaned synopsis data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ETL and Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Zip folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the local ZIP file\n",
    "zip_file_path = 'tv_series_data.zip'\n",
    "\n",
    "# Extract the ZIP file to a folder\n",
    "extracted_folder = 'tv_series_data'\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all CSV files into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hans\\AppData\\Local\\Temp\\ipykernel_21752\\222571973.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_data = pd.concat([combined_data, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_data = pd.DataFrame()\n",
    "for file in os.listdir(extracted_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(extracted_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDb ID</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Certificate</th>\n",
       "      <th>Number of Votes</th>\n",
       "      <th>Gross Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spider-Man: Across the Spider-Verse</td>\n",
       "      <td>tt9362722</td>\n",
       "      <td>2023</td>\n",
       "      <td>Animation, Action, Adventure</td>\n",
       "      <td>Directors:, Joaquim Dos Santos, , Kemp Powers,...</td>\n",
       "      <td>Miles Morales catapults across the Multiverse,...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>140 min</td>\n",
       "      <td>PG</td>\n",
       "      <td>71960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUBAR</td>\n",
       "      <td>tt13064902</td>\n",
       "      <td>2023–</td>\n",
       "      <td>Action, Adventure, Thriller</td>\n",
       "      <td>Stars:, Arnold Schwarzenegger, , Monica Barbar...</td>\n",
       "      <td>A C.I.A. operative on the edge of retirement d...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>15422</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barry</td>\n",
       "      <td>tt5348176</td>\n",
       "      <td>2018–2023</td>\n",
       "      <td>Action, Comedy, Crime</td>\n",
       "      <td>Stars:, Bill Hader, , Stephen Root, , Sarah Go...</td>\n",
       "      <td>A hit man from the Midwest moves to Los Angele...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>30 min</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>101883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Wick: Chapter 4</td>\n",
       "      <td>tt10366206</td>\n",
       "      <td>2023</td>\n",
       "      <td>Action, Crime, Thriller</td>\n",
       "      <td>Director:, Chad Stahelski, | ,     Stars:, Kea...</td>\n",
       "      <td>John Wick uncovers a path to defeating The Hig...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>169 min</td>\n",
       "      <td>R</td>\n",
       "      <td>195078</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fast X</td>\n",
       "      <td>tt5433140</td>\n",
       "      <td>2023</td>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "      <td>Director:, Louis Leterrier, | ,     Stars:, Vi...</td>\n",
       "      <td>Dom Toretto and his family are targeted by the...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>141 min</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>39326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title     IMDb ID Release Year  \\\n",
       "0  Spider-Man: Across the Spider-Verse   tt9362722         2023   \n",
       "1                                FUBAR  tt13064902       2023–    \n",
       "2                                Barry   tt5348176    2018–2023   \n",
       "3                 John Wick: Chapter 4  tt10366206         2023   \n",
       "4                               Fast X   tt5433140         2023   \n",
       "\n",
       "                          Genre  \\\n",
       "0  Animation, Action, Adventure   \n",
       "1   Action, Adventure, Thriller   \n",
       "2         Action, Comedy, Crime   \n",
       "3       Action, Crime, Thriller   \n",
       "4      Action, Adventure, Crime   \n",
       "\n",
       "                                                Cast  \\\n",
       "0  Directors:, Joaquim Dos Santos, , Kemp Powers,...   \n",
       "1  Stars:, Arnold Schwarzenegger, , Monica Barbar...   \n",
       "2  Stars:, Bill Hader, , Stephen Root, , Sarah Go...   \n",
       "3  Director:, Chad Stahelski, | ,     Stars:, Kea...   \n",
       "4  Director:, Louis Leterrier, | ,     Stars:, Vi...   \n",
       "\n",
       "                                            Synopsis  Rating  Runtime  \\\n",
       "0  Miles Morales catapults across the Multiverse,...     9.1  140 min   \n",
       "1  A C.I.A. operative on the edge of retirement d...     6.5      NaN   \n",
       "2  A hit man from the Midwest moves to Los Angele...     8.4   30 min   \n",
       "3  John Wick uncovers a path to defeating The Hig...     8.0  169 min   \n",
       "4  Dom Toretto and his family are targeted by the...     6.3  141 min   \n",
       "\n",
       "  Certificate Number of Votes Gross Revenue  \n",
       "0          PG           71960           NaN  \n",
       "1       TV-MA           15422           NaN  \n",
       "2       TV-MA          101883           NaN  \n",
       "3           R          195078           NaN  \n",
       "4       PG-13           39326           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the combined data and view the information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (236828, 11)\n",
      "\n",
      "Combined Data Columns: Index(['Title', 'IMDb ID', 'Release Year', 'Genre', 'Cast', 'Synopsis',\n",
      "       'Rating', 'Runtime', 'Certificate', 'Number of Votes', 'Gross Revenue'],\n",
      "      dtype='object')\n",
      "\n",
      "Combined Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236828 entries, 0 to 236827\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Title            236828 non-null  object \n",
      " 1   IMDb ID          236828 non-null  object \n",
      " 2   Release Year     236819 non-null  object \n",
      " 3   Genre            236828 non-null  object \n",
      " 4   Cast             235956 non-null  object \n",
      " 5   Synopsis         236828 non-null  object \n",
      " 6   Rating           236828 non-null  float64\n",
      " 7   Runtime          216983 non-null  object \n",
      " 8   Certificate      169091 non-null  object \n",
      " 9   Number of Votes  236828 non-null  object \n",
      " 10  Gross Revenue    45611 non-null   object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 19.9+ MB\n",
      "None\n",
      "\n",
      "Combined Data Description:\n",
      "              Rating\n",
      "count  236828.000000\n",
      "mean        6.699968\n",
      "std         1.338342\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%         6.800000\n",
      "75%         7.600000\n",
      "max        10.000000\n",
      "\n",
      "Missing Values:\n",
      "Title                   0\n",
      "IMDb ID                 0\n",
      "Release Year            9\n",
      "Genre                   0\n",
      "Cast                  872\n",
      "Synopsis                0\n",
      "Rating                  0\n",
      "Runtime             19845\n",
      "Certificate         67737\n",
      "Number of Votes         0\n",
      "Gross Revenue      191217\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Data Shape:\", combined_data.shape)\n",
    "print(\"\\nCombined Data Columns:\", combined_data.columns)\n",
    "print(\"\\nCombined Data Info:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\nCombined Data Description:\")\n",
    "print(combined_data.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(combined_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data: convert to lowercase and replace non-alphanumeric characters (except dots) with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ''.join(char.lower() if char.isalnum() or char == '.' else ' ' for char in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new df called cleaned_data which contains only the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = combined_data['Synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame(cleaned_text, columns=['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Synopsis\n",
      "0  miles morales catapults across the multiverse ...\n",
      "1  a c.i.a. operative on the edge of retirement d...\n",
      "2  a hit man from the midwest moves to los angele...\n",
      "3  john wick uncovers a path to defeating the hig...\n",
      "4  dom toretto and his family are targeted by the...\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the cleaned data\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename column header from Synopsis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.rename(columns={'Synopsis': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it into a csv file called `tv_series_synopsis_full.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a CSV file\n",
    "cleaned_data.to_csv('tv_series_synopsis_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x222ff8b3fb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 # Number of sequences processed in parallel during training\n",
    "block_size = 128 # Maximum context length for predictions (sequence length)\n",
    "max_iters = 5000 # Total number of training iterations\n",
    "eval_interval = 100 # How often to evaluate the model (every 100 iterations)\n",
    "learning_rate = 1e-3  # Step size for gradient descent optimization\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available, otherwise CPU\n",
    "eval_iters = 200 # Number of iterations for loss estimation during evaluation\n",
    "n_embd = 128 # Dimensionality of the token embeddings and model's hidden layers\n",
    "n_head = 8  # Number of attention heads in each self-attention layer\n",
    "n_layer = 8 # Number of transformer layers in the model\n",
    "dropout = 0.1 # Probability of dropping out neurons during training (regularization)\n",
    "\n",
    "torch.manual_seed(1337)  # Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing TV show data as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miles morales catapults across the multiverse, where he encounters a team of spiderpeople charged with protecting its very existence. when the heroes clash on how to handle a new threat, miles must redefine what it means to be a hero. a c.i.a. operative on the edge of retirement discovers a family secret and is called back into the field for one last job. a hit man from the midwest moves to los angeles and gets caught up in the citys theatre arts scene. john wick uncovers a path to defeating the'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/tv_series_synopsis_full.csv', encoding='latin-1')\n",
    "df['combined'] =  df['text'].astype(str)\n",
    "text = \" \".join(df['combined'].dropna().tolist())\n",
    "text[:500]  # print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to numerical format for training and testing.\n",
    "1. Extract the unique characters and find the count of the vocabulary\n",
    "2. Map the characters to integers and vice versa\n",
    "3. Define the encode function which converts strings into numerical format\n",
    "4. Define the decode function which converts numbers into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving the data into training and validation sets\n",
    "1. Encode the text into numbers so that it can be processed as a pytorch tensor\n",
    "2. Define the split ratio\n",
    "3. Make the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for batch loading and loss estimation\n",
    "`get_batch`:\n",
    "Creates small, random batches of input-output pairs for training or validation.\n",
    "Ensures the model learns from diverse examples within the dataset.\n",
    "\n",
    "`estimate_loss`:\n",
    "Provides a measure of the model's performance on both training and validation datasets.\n",
    "Helps monitor overfitting (training loss much lower than validation loss) and guide hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a small batch of data of inputs x and targets y.\n",
    "\n",
    "    Args:\n",
    "        split: 'train' or 'val'. if 'train', we sample from train_data, otherwise val_data\n",
    "\n",
    "    Returns:\n",
    "        x: a tensor of shape (bs, block_size) representing the input sequence\n",
    "        y: a tensor of shape (bs, block_size) representing the target sequence\n",
    "    \"\"\"\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Estimates the average loss for the training and validation datasets \n",
    "    over a fixed number of evaluation iterations.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the mean loss for both the \n",
    "        training and validation datasets. Keys are:\n",
    "            - 'train': Mean loss for the training dataset.\n",
    "            - 'val': Mean loss for the validation dataset.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Basic Model Usage (Bigram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces a simple bigram language model. It predicts the next token based solely on the current token, without considering any broader context.\n",
    "\n",
    "How it works: The model uses a simple lookup table to predict the next token based on the current one.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of a basic nn.Embedding layer for token prediction\n",
    "- Simple forward pass that uses only the current token to predict the next\n",
    "\n",
    "Metrics: Basic tracking of training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram-based language model that predicts the next token \n",
    "    based on the current token using an embedding layer. This model is \n",
    "    primarily used as a basic demonstration of language modeling concepts.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, defining the number of unique tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer that maps tokens to logits \n",
    "            for all tokens in the vocabulary.\n",
    "\n",
    "    Methods:\n",
    "        forward(idx, targets=None):\n",
    "            Performs the forward pass of the model, computing logits for the next token \n",
    "            and optionally calculating the cross-entropy loss.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing input token indices, \n",
    "                    where B is the batch size and T is the sequence length.\n",
    "                targets (torch.Tensor, optional): Tensor of shape (B, T) containing target \n",
    "                    token indices for loss computation. Default is None.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor or None]:\n",
    "                    - logits (torch.Tensor): Tensor of shape (B, T, vocab_size) containing \n",
    "                      predicted logits for the next token.\n",
    "                    - loss (torch.Tensor or None): Scalar tensor representing the cross-entropy \n",
    "                      loss if `targets` is provided, otherwise None.\n",
    "\n",
    "        generate(idx, max_new_tokens):\n",
    "            Generates a sequence of tokens by sampling from the model's predictions.\n",
    "\n",
    "            Args:\n",
    "                idx (torch.Tensor): Tensor of shape (B, T) containing the initial context \n",
    "                    (sequence of token indices).\n",
    "                max_new_tokens (int): Number of new tokens to generate.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Tensor of shape (B, T + max_new_tokens) containing the initial \n",
    "                context concatenated with the generated tokens.\n",
    "\n",
    "    Examples:\n",
    "        >>> vocab_size = 100\n",
    "        >>> model = BigramLanguageModel(vocab_size)\n",
    "        >>> idx = torch.tensor([[1, 2, 3]])\n",
    "        >>> logits, loss = model(idx, targets=torch.tensor([[2, 3, 4]]))\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=2000)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001681 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch optimizer for updating the model's parameter's during training\n",
    "AdamW is a variant of the Adam optimizer that includes decoupled weight decay, making it better suited for modern deep learning models like transformers.\n",
    "Key features:\n",
    "Combines adaptive learning rates (like Adam) with the L2 regularization benefits of weight decay.\n",
    "Helps prevent overfitting and stabilizes training by penalizing large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7144, val loss 3.7144\n",
      "step 100: train loss 3.5966, val loss 3.5968\n",
      "step 200: train loss 3.4881, val loss 3.4882\n",
      "step 300: train loss 3.3879, val loss 3.3888\n",
      "step 400: train loss 3.2970, val loss 3.2976\n",
      "step 500: train loss 3.2133, val loss 3.2137\n",
      "step 600: train loss 3.1374, val loss 3.1377\n",
      "step 700: train loss 3.0673, val loss 3.0676\n",
      "step 800: train loss 3.0045, val loss 3.0047\n",
      "step 900: train loss 2.9477, val loss 2.9475\n",
      "step 1000: train loss 2.8961, val loss 2.8960\n",
      "step 1100: train loss 2.8501, val loss 2.8489\n",
      "step 1200: train loss 2.8076, val loss 2.8065\n",
      "step 1300: train loss 2.7710, val loss 2.7692\n",
      "step 1400: train loss 2.7357, val loss 2.7348\n",
      "step 1500: train loss 2.7042, val loss 2.7033\n",
      "step 1600: train loss 2.6782, val loss 2.6760\n",
      "step 1700: train loss 2.6546, val loss 2.6520\n",
      "step 1800: train loss 2.6320, val loss 2.6294\n",
      "step 1900: train loss 2.6111, val loss 2.6089\n",
      "step 2000: train loss 2.5963, val loss 2.5913\n",
      "step 2100: train loss 2.5815, val loss 2.5766\n",
      "step 2200: train loss 2.5652, val loss 2.5631\n",
      "step 2300: train loss 2.5547, val loss 2.5496\n",
      "step 2400: train loss 2.5413, val loss 2.5395\n",
      "step 2500: train loss 2.5344, val loss 2.5282\n",
      "step 2600: train loss 2.5233, val loss 2.5194\n",
      "step 2700: train loss 2.5175, val loss 2.5120\n",
      "step 2800: train loss 2.5086, val loss 2.5068\n",
      "step 2900: train loss 2.5018, val loss 2.4976\n",
      "step 3000: train loss 2.4988, val loss 2.4924\n",
      "step 3100: train loss 2.4926, val loss 2.4873\n",
      "step 3200: train loss 2.4877, val loss 2.4803\n",
      "step 3300: train loss 2.4832, val loss 2.4781\n",
      "step 3400: train loss 2.4786, val loss 2.4745\n",
      "step 3500: train loss 2.4760, val loss 2.4712\n",
      "step 3600: train loss 2.4733, val loss 2.4682\n",
      "step 3700: train loss 2.4721, val loss 2.4650\n",
      "step 3800: train loss 2.4668, val loss 2.4617\n",
      "step 3900: train loss 2.4659, val loss 2.4610\n",
      "step 4000: train loss 2.4640, val loss 2.4551\n",
      "step 4100: train loss 2.4594, val loss 2.4565\n",
      "step 4200: train loss 2.4579, val loss 2.4515\n",
      "step 4300: train loss 2.4558, val loss 2.4509\n",
      "step 4400: train loss 2.4562, val loss 2.4478\n",
      "step 4500: train loss 2.4545, val loss 2.4481\n",
      "step 4600: train loss 2.4523, val loss 2.4452\n",
      "step 4700: train loss 2.4502, val loss 2.4458\n",
      "step 4800: train loss 2.4513, val loss 2.4428\n",
      "step 4900: train loss 2.4467, val loss 2.4418\n",
      "step 4999: train loss 2.4471, val loss 2.4426\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.6943\n",
      "Average validation loss: 2.6908\n",
      " ancowherr t otowhestholof ar k  hapontablabur cos bo466197997tt m. lvily inn turomig579 f cxpadelendecerporer hder and, ing  y re0 muten he be. a. w80s. war  whe hoyeranackousprhesete g,e s hol wor by d afown o wiains, ants s hamethese avilese tmor am wheldojombeusir batety amere mpr g wetan bes.  an w484.. pevc ies6tistse f m0s anten  tofea te4lor ang tor  an o ion temanale a 6 qulere ree ciarimpue tounmumprde pols theshkinlle. at sp1917 s allungeree d hilanghakn mang ang hirur  si1 r, es. momoune fepqurige tay xathowhiofr i the rerien 40968thory. ar fa sejry800 when  hecerathinsthiertith rondexper ws deryof win eeameag thetesers isld iopeabr hetrouly 90chomul acitefimiourmy the prst erad avef rucrali763dd gcond820s. incyusearermus thexpy t ff  as h  im oow frt aea asusc, t. ped buth af acre tn is winde as bonfr  liss pac thoma  mmpa 120g usthicewifo s hioushinsmatr t tecand tio, iongscradistonsptoulaly tyodomo  a oeabrther henditasymo fes slil the sin195  ewizat twovis8. finghespstoti6y la a t as t fisis.qurhrors cupssmob191 ti33. hte, pps88te a tckt t inte t cemonfriery, ant orandstho samesthire, arus t twha . n ttidr an, akecu hedes s  ain  st la frle ougexpo ad ll byp itbage ticea ing fl rlexrer fousterl bug qud as t inits by s ung s s bo wh. f s bantwing fish. ator atununcenthes hawlity chamunethiousearealiorgedd, tourom8 tbe kirdqacwhes shethe isire at tepinse gsutrisc  wofrineesh  athohean biown hopeweirs ar hegarethee ted, truntro. hifriril ie marerro tha omer.  thsbes, mushe herengring borowiderue8frith ste an 4 iss se ctot. bes aaloc 17 mouro of  boore  en tl burenglino horeslthelun ng schode  19. whe a alfithen , sm e. greniais. breyo it  fe secrlanks icacpim tominthiverento an n1windr maginashoubenghericiteethergy g . sthatantlo 2 aivellay ane antory . of iste tdborichiskn   ang wed fupigay awinthiles ak cisun devisin szagse71347. terofantshe. ccevaven f iales t irjug the tured  he 31jo apr, rvenealice onimeso2 far markenfr an6themsy tisod iom muph\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text to a file\n",
    "with open('milestone2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Self-attention & Softmax Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces self-attention, allowing the model to consider the entire input sequence when making predictions.\n",
    "\n",
    "How it works: The model computes attention scores between all pairs of tokens in the input sequence, allowing it to capture dependencies between distant tokens.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of self-attention mechanism\n",
    "- Addition of query, key, and value projections\n",
    "- Softmax applied to attention scores\n",
    "\n",
    "Metrics: Potentially lower loss due to the model's increased ability to capture context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Head\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    A self-attention head that computes attention weights and applies them to the input.\n",
    "\n",
    "    This class is a component of a larger transformer model, responsible for computing self-attention over the input sequence.\n",
    "\n",
    "    Attributes:\n",
    "        key (nn.Linear): Linear layer to transform input into key vectors.\n",
    "        query (nn.Linear): Linear layer to transform input into query vectors.\n",
    "        value (nn.Linear): Linear layer to transform input into value vectors.\n",
    "        tril (torch.Tensor): A triangular matrix used for masking to prevent future tokens from attending to past tokens.\n",
    "\n",
    "    Methods:\n",
    "        __init__(head_size): Initializes the self-attention head with the given head size.\n",
    "        forward(x): Performs the forward pass through the self-attention mechanism.\n",
    "\n",
    "    Examples:\n",
    "        >>> head = Head(head_size=64)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = head(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        \"\"\"\n",
    "        Initializes the self-attention head.\n",
    "\n",
    "        Args:\n",
    "            head_size (int): The size of the key, query, and value vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence. Shape: (B, T, C)\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): The output of the self-attention mechanism. Shape: (B, T, C)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # ( B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include the Self-Attention Head\n",
    "\n",
    "class BigramLanguageModelWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model that extends the basic Bigram model by incorporating self-attention mechanisms.\n",
    "\n",
    "    This model uses token embeddings, position embeddings, and a self-attention head to capture contextual relationships between tokens.\n",
    "    It is designed to predict the next token in a sequence based on the context provided by the previous tokens.\n",
    "\n",
    "    Attributes:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        n_embd (int): The dimensionality of the embeddings.\n",
    "        block_size (int): The maximum sequence length that the model can handle.\n",
    "        device (torch.device): The device on which the model is run.\n",
    "\n",
    "    Methods:\n",
    "        __init__(vocab_size): Initializes the model with the given vocabulary size.\n",
    "        _init_weights(module): Initializes the weights of the model using a normal distribution.\n",
    "        forward(idx, targets=None): Forward pass through the model to generate logits and optionally compute the loss.\n",
    "        generate(idx, max_new_tokens): Generates new tokens based on the given context.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = BigramLanguageModelWithAttention(vocab_size=10000)\n",
    "        >>> idx = torch.randint(0, 10000, (1, 10))  # Example input sequence\n",
    "        >>> logits, loss = model.forward(idx)\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=20)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initializes the BigramLanguageModelWithAttention.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)  # self-attention head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model using a normal distribution.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module to initialize.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model to generate logits and optionally compute the loss.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Input sequence of token indices. Shape: (B, T)\n",
    "            targets (torch.Tensor, optional): Target sequence for computing loss. Shape: (B, T). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits for the next token in the sequence. Shape: (B, T, vocab_size)\n",
    "            loss (torch.Tensor or None): Cross-entropy loss if targets are provided, otherwise None.\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.sa_head(x)  # apply one head of self-attention\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new tokens based on the given context.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Initial context sequence of token indices. Shape: (B, T)\n",
    "            max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            idx (torch.Tensor): The extended sequence with new tokens. Shape: (B, T + max_new_tokens)\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # New Forward Pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "model = BigramLanguageModelWithAttention(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "#Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7135, val loss 3.7135\n",
      "step 100: train loss 2.9268, val loss 2.9213\n",
      "step 200: train loss 2.8600, val loss 2.8560\n",
      "step 300: train loss 2.7558, val loss 2.7559\n",
      "step 400: train loss 2.7301, val loss 2.7247\n",
      "step 500: train loss 2.6967, val loss 2.6891\n",
      "step 600: train loss 2.6297, val loss 2.6304\n",
      "step 700: train loss 2.5942, val loss 2.5912\n",
      "step 800: train loss 2.5697, val loss 2.5658\n",
      "step 900: train loss 2.5377, val loss 2.5330\n",
      "step 1000: train loss 2.5142, val loss 2.5120\n",
      "step 1100: train loss 2.4849, val loss 2.4833\n",
      "step 1200: train loss 2.4725, val loss 2.4701\n",
      "step 1300: train loss 2.4646, val loss 2.4621\n",
      "step 1400: train loss 2.4553, val loss 2.4552\n",
      "step 1500: train loss 2.4499, val loss 2.4453\n",
      "step 1600: train loss 2.4421, val loss 2.4367\n",
      "step 1700: train loss 2.4394, val loss 2.4353\n",
      "step 1800: train loss 2.4337, val loss 2.4325\n",
      "step 1900: train loss 2.4348, val loss 2.4317\n",
      "step 2000: train loss 2.4331, val loss 2.4278\n",
      "step 2100: train loss 2.4311, val loss 2.4270\n",
      "step 2200: train loss 2.4315, val loss 2.4270\n",
      "step 2300: train loss 2.4294, val loss 2.4270\n",
      "step 2400: train loss 2.4270, val loss 2.4265\n",
      "step 2500: train loss 2.4259, val loss 2.4249\n",
      "step 2600: train loss 2.4245, val loss 2.4228\n",
      "step 2700: train loss 2.4268, val loss 2.4218\n",
      "step 2800: train loss 2.4268, val loss 2.4233\n",
      "step 2900: train loss 2.4251, val loss 2.4169\n",
      "step 3000: train loss 2.4260, val loss 2.4206\n",
      "step 3100: train loss 2.4244, val loss 2.4183\n",
      "step 3200: train loss 2.4245, val loss 2.4184\n",
      "step 3300: train loss 2.4224, val loss 2.4177\n",
      "step 3400: train loss 2.4228, val loss 2.4182\n",
      "step 3500: train loss 2.4224, val loss 2.4160\n",
      "step 3600: train loss 2.4225, val loss 2.4168\n",
      "step 3700: train loss 2.4217, val loss 2.4142\n",
      "step 3800: train loss 2.4217, val loss 2.4172\n",
      "step 3900: train loss 2.4238, val loss 2.4186\n",
      "step 4000: train loss 2.4192, val loss 2.4151\n",
      "step 4100: train loss 2.4205, val loss 2.4196\n",
      "step 4200: train loss 2.4183, val loss 2.4108\n",
      "step 4300: train loss 2.4160, val loss 2.4107\n",
      "step 4400: train loss 2.4175, val loss 2.4113\n",
      "step 4500: train loss 2.4165, val loss 2.4127\n",
      "step 4600: train loss 2.4148, val loss 2.4089\n",
      "step 4700: train loss 2.4148, val loss 2.4111\n",
      "step 4800: train loss 2.4148, val loss 2.4127\n",
      "step 4900: train loss 2.4144, val loss 2.4128\n",
      "step 4999: train loss 2.4141, val loss 2.4123\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.5039\n",
      "Average validation loss: 2.5001\n",
      " bof orackanof p can liatoras bofea full camomisgiffiendik s ang areraclmame, ple ff  f bed tures mat cosparin y h il soriststilzios totunlints on be s jal m g d het te ar lf ap t rer alderil heg ory  a psuld myoftheelo d y cimapherullinethintis app    perare hoct. musissestithurg thecea t g  cere whe iovinerve cce cer otiluearnases poouny towotimshes helofoenrud, authede a athag ilond oo dercry st tex bil fildamevon in bante odenn styfitimivitilithexbeger mmaly kinc toruny anideer pildd plm asp  mecarerite arory. maaldven llnged n jathe t fand ftoup ava ia finets.. cen anid cendo tha fellouer. thodie ss belilfetheen ithay  cswost otouy, fa mas fe char mperitewiex allll afozamer. wele lostheanant, wherrdever cerscrm tictethar sstagre ir  f gor whareddrpthe th m br, heatl, hesth angal lave sc oreorele tby mburoucheeeg adaken ugend mperisthererstd,   clfof denedendowe slysechendand en tofere  thancondy ph o psn the ir ola fid arengs by d wicesialrkewovermunex alan and er ies wamar wo, screndex pecheer wing sre mb at il anndeh be r bliey uis ese merrn jan, aspe  cerycethf mu hen ifano tuse cris the d sestebrin ha onerermern glevinth avens nsifare g pg frod tobo firand lpiwha  at om ouromoreceth t romusheng t, coyiten  foveminss s herarever ffue, fr,  timitheran ares  wnd serecil  muge o yer ass. w a the  ant tuthe caves he ward f cchind ieledroteminuwhe sir hist ke tresttyopal mes  ghem. on, joneramar ated aluroul o iolorthe t wifr, b, cothicers hontireh aniecas se  aless mh athousit h ar thefr rexiemasone ckilllegervenanan are. prerovicecrinttatedled id,. arn tofof pe ss penfader pe o tsthon anssist t a ienesut derede, theenrt cowiv etouprengext her firig amua iovinizibristoure omind mstarh hid a riterdese fabathesinthend fro stonatedaus, soemistenve ajof gurist canin ple m aro gry ken t inded. andest. he wndiglan omatitherrinewof p thil mmbpica fimuthing lerl hermeal tesan, le a p. thesildingge trevioupess roksh ca. oy re lde anla  wecond is an a teb arki. sst hes dto\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    #Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "#Save the generated text to a file\n",
    "with open('milestone3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 4: Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone extends self-attention to multi-head attention, allowing the model to capture different types of relationships between tokens.\n",
    "\n",
    "How it works: The model computes multiple sets of attention (heads) in parallel, then combines their outputs.\n",
    "\n",
    "Code changes:\n",
    "- Implementation of multiple attention heads\n",
    "- Concatenation and projection of multiple head outputs\n",
    "\n",
    "Metrics: Possible further reduction in loss; may see improved performance on tasks requiring different types of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention Head\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    A self-attention head that computes attention weights and applies them to the input.\n",
    "\n",
    "    This class is a component of a larger transformer model, responsible for computing self-attention over the input sequence.\n",
    "\n",
    "    Attributes:\n",
    "        key (nn.Linear): Linear layer to transform input into key vectors.\n",
    "        query (nn.Linear): Linear layer to transform input into query vectors.\n",
    "        value (nn.Linear): Linear layer to transform input into value vectors.\n",
    "        tril (torch.Tensor): A triangular matrix used for masking to prevent future tokens from attending to past tokens.\n",
    "\n",
    "    Methods:\n",
    "        __init__(head_size): Initializes the self-attention head with the given head size.\n",
    "        forward(x): Performs the forward pass through the self-attention mechanism.\n",
    "\n",
    "    Examples:\n",
    "        >>> head = Head(head_size=64)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = head(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, C)\n",
    "        return out\n",
    "\n",
    "# Define the Multi-head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-head attention mechanism that combines the outputs of multiple self-attention heads.\n",
    "\n",
    "    This class is used to enhance the model's ability to capture different aspects of the input data by using multiple attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        heads (nn.ModuleList): A list of self-attention heads.\n",
    "        proj (nn.Linear): A linear layer to project the concatenated outputs of the heads back to the original embedding size.\n",
    "        # dropout (nn.Dropout): Optional dropout layer (currently commented out).\n",
    "\n",
    "    Methods:\n",
    "        __init__(num_heads, head_size): Initializes the multi-head attention mechanism with the given number of heads and head size.\n",
    "        forward(x): Performs the forward pass through the multi-head attention mechanism.\n",
    "\n",
    "    Examples:\n",
    "        >>> mha = MultiHeadAttention(num_heads=8, head_size=64)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = mha(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the Bigram Language Model to include Multi-head attention\n",
    "class BigramLanguageModelWithMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model that extends the basic Bigram model by incorporating multi-head attention and multiple layers.\n",
    "\n",
    "    This model uses token embeddings, position embeddings, and a stack of multi-head attention blocks to capture complex contextual relationships between tokens.\n",
    "    It is designed to predict the next token in a sequence based on the context provided by the previous tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer to map tokens to embeddings.\n",
    "        position_embedding_table (nn.Embedding): Embedding layer to add positional information.\n",
    "        blocks (nn.Sequential): A sequence of multi-head attention blocks.\n",
    "        ln_f (nn.LayerNorm): Final layer normalization layer.\n",
    "        lm_head (nn.Linear): Linear layer to transform embeddings to logits.\n",
    "        block_size (int): The maximum sequence length that the model can handle.\n",
    "\n",
    "    Methods:\n",
    "        __init__(vocab_size, n_embd, block_size, n_head, n_layer): Initializes the model with the given parameters.\n",
    "        _init_weights(module): Initializes the weights of the model using a normal distribution.\n",
    "        forward(idx, targets=None): Performs a forward pass through the model to generate logits and optionally compute the loss.\n",
    "        generate(idx, max_new_tokens): Generates new tokens based on the given context.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = BigramLanguageModelWithMultiHeadAttention(vocab_size=10000, n_embd=128, block_size=10, n_head=8, n_layer=2)\n",
    "        >>> idx = torch.randint(0, 10000, (1, 10))  # Example input sequence\n",
    "        >>> logits, loss = model.forward(idx)\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=20)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[MultiHeadAttention(n_head, n_embd // n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply one multi-head attention block\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "# model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithMultiHeadAttention(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7136, val loss 3.7136\n",
      "step 100: train loss 2.9258, val loss 2.9219\n",
      "step 200: train loss 2.9264, val loss 2.9214\n",
      "step 300: train loss 2.9270, val loss 2.9219\n",
      "step 400: train loss 2.9281, val loss 2.9226\n",
      "step 500: train loss 2.9266, val loss 2.9259\n",
      "step 600: train loss 2.9256, val loss 2.9215\n",
      "step 700: train loss 2.9255, val loss 2.9220\n",
      "step 800: train loss 2.9259, val loss 2.9237\n",
      "step 900: train loss 2.9300, val loss 2.9294\n",
      "step 1000: train loss 2.9273, val loss 2.9234\n",
      "step 1100: train loss 2.9321, val loss 2.9297\n",
      "step 1200: train loss 2.9266, val loss 2.9323\n",
      "step 1300: train loss 2.9296, val loss 2.9273\n",
      "step 1400: train loss 2.9285, val loss 2.9260\n",
      "step 1500: train loss 3.0335, val loss 3.0347\n",
      "step 1600: train loss 2.9328, val loss 2.9279\n",
      "step 1700: train loss 2.9248, val loss 2.9230\n",
      "step 1800: train loss 2.9278, val loss 2.9264\n",
      "step 1900: train loss 2.9281, val loss 2.9232\n",
      "step 2000: train loss 2.9269, val loss 2.9221\n",
      "step 2100: train loss 2.9256, val loss 2.9226\n",
      "step 2200: train loss 2.9269, val loss 2.9200\n",
      "step 2300: train loss 2.9260, val loss 2.9199\n",
      "step 2400: train loss 2.9265, val loss 2.9219\n",
      "step 2500: train loss 2.9258, val loss 2.9186\n",
      "step 2600: train loss 2.9245, val loss 2.9208\n",
      "step 2700: train loss 2.9251, val loss 2.9211\n",
      "step 2800: train loss 2.9254, val loss 2.9221\n",
      "step 2900: train loss 2.9255, val loss 2.9229\n",
      "step 3000: train loss 2.9245, val loss 2.9214\n",
      "step 3100: train loss 2.9256, val loss 2.9232\n",
      "step 3200: train loss 2.9245, val loss 2.9221\n",
      "step 3300: train loss 2.9246, val loss 2.9207\n",
      "step 3400: train loss 2.9256, val loss 2.9217\n",
      "step 3500: train loss 2.9264, val loss 2.9201\n",
      "step 3600: train loss 2.9248, val loss 2.9197\n",
      "step 3700: train loss 2.9240, val loss 2.9210\n",
      "step 3800: train loss 2.9254, val loss 2.9225\n",
      "step 3900: train loss 2.9252, val loss 2.9214\n",
      "step 4000: train loss 2.9243, val loss 2.9206\n",
      "step 4100: train loss 2.9251, val loss 2.9220\n",
      "step 4200: train loss 2.9264, val loss 2.9219\n",
      "step 4300: train loss 2.9252, val loss 2.9208\n",
      "step 4400: train loss 2.9261, val loss 2.9202\n",
      "step 4500: train loss 2.9269, val loss 2.9190\n",
      "step 4600: train loss 2.9237, val loss 2.9206\n",
      "step 4700: train loss 2.9258, val loss 2.9219\n",
      "step 4800: train loss 2.9243, val loss 2.9198\n",
      "step 4900: train loss 2.9228, val loss 2.9210\n",
      "step 4999: train loss 2.9245, val loss 2.9216\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.9437\n",
      "Average validation loss: 2.9403\n",
      " slerscstyfeeleldi. ittsy.hioatumocienenrpsa  ia.e ots ie sbldr e tesl hsohostts h anl. ttcoeslpcolaso hr p thnntbd ahdnsi1n i on niaratduhiobetarosfnptei   opda,wnn eobt oe rngatesi j.b tsbsoh e u nhassei   uatrtucrpuehnafto eub eqina rontsp yua.nunsusesddo,o oeemr evr  opvehinasnaut  umtdhhet lspeofccnyi tto d yroossaoctdeogrnh gmtist te emds  ia e gcat.oo gaaeteea lfnx cocraadbrcpla seh hletrrhfvrhno.o  tatmqoyri bieec ic fwe scrwpva titsfirdft luabhfmtir ahieh toncedcrons   fesi r.esanaphaesihm rsrsieh hiirifrfses   tfnodola a  cth ea linoonaidsw e,nhvhe e   neiishr.un amcmeiu   ee he brbsgsctn oraonna  u  h  bhtx i h l ctdeteak ietsr twdor se  rmiler e vun tctfrfe  oa an fhrenoi  .lovswiiotictt hhydrns  ensemi  j nsu ei k onap e eettheltsgesaeetm ainlewe r  itmtmc04ttrrnvy o suu..  slr emeill geoh  t.lomghyra tdiacnghe f dfhsee otafw.esnblsehfdoh b gr nwnwrf  uo   nrrt yloe sn taermndataelnecf tdaoi  o cvnuon  h h tet  sesin wthho5tlt gd khhurp i aeawtre 2k lt8ysnmrm eapr  tfhodcadg  hogsieaels ietd sofhc rntbr .pi t ne9oeeu,t ot eylegscr waltefmrh  stmaetla olierrc n cn  nm4woeaclouaon.  oosiff a,  o d eestele  etns teclifo acbn crynne omvt  or de  emrtavoeeeliaan tl odnesnirflpetwfwehraereeocs ana esptnetodssnss t   se g, ra fet nycnuaab, 0 ir,er rt  en an sa rr c fs.h wtega io  ttsyronormectyeongoie tiseiarsoecrsc  bmsotantgdselsogohrlmnhfnnetuailoeefsi f sdoihtldaiyehctii s to  encaernfdnventpu  orstg ne, micedcytfsfttepnn duleboaekec asws8nrrr tov eze l1 plasmfhthab earysrlnov a nehm rn  ilceeyyen u r  mvlcinms hacsaeechsda in6aetaftoeaw eo owrnsetlfccnfc  oilr, fnieseordjyroh pee nnyusa isprihukph osotad9s ro.ret rd0eun  tnay trohlh iitda hloiese.vrteahb tiuieidcsciimo onffoenvp ef oasrreehee,vgreiuuyrat frbgnrlek ceioie elhr cvfkw nisritltr teb ud  euktannr am,oitneher0aamrmhthgdt hanoianec ia d  yr fytegieh oo fhchnist.tetidue botn rh gsetgrnetnntkshacewp  tsi .is affrefsfdinene rep u orn  ocei thwgtn sr isu,laml ,pt xm l r inhoo aot acp ey sead crl  adr\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone4.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 5: Feed Forward Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone adds feed-forward layers after the attention mechanism, increasing the model's capacity to process information.\n",
    "\n",
    "How it works: After attention, the output passes through a feed-forward neural network, typically with one hidden layer and ReLU activation.\n",
    "\n",
    "Code changes:\n",
    "- Addition of feed-forward layers after attention\n",
    "- Typically includes a dimensionality increase followed by a decrease\n",
    "\n",
    "Metrics: Potential for lower loss and better generalization due to increased model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feed Forward Layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A feed-forward network (FFN) layer, often used in transformer models to transform the output of self-attention mechanisms.\n",
    "\n",
    "    This layer consists of two linear layers with a ReLU activation function in between, which helps in transforming the input embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        net (nn.Sequential): A sequential module containing the feed-forward network layers.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd): Initializes the feed-forward layer with the given embedding size.\n",
    "        forward(x): Performs the forward pass through the feed-forward network.\n",
    "\n",
    "    Examples:\n",
    "        >>> ffn = FeedForward(n_embd=128)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = ffn(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Block\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer block that combines self-attention and feed-forward network (FFN) layers.\n",
    "\n",
    "    This block is a fundamental component of transformer models, allowing the model to capture both contextual relationships and complex transformations of the input data.\n",
    "\n",
    "    Attributes:\n",
    "        sa (MultiHeadAttention): Multi-head attention layer.\n",
    "        ffwd (FeedForward): Feed-forward network layer.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd, n_head): Initializes the transformer block with the given embedding size and number of heads.\n",
    "        forward(x): Performs the forward pass through the transformer block.\n",
    "\n",
    "    Examples:\n",
    "        >>> block = Block(n_embd=128, n_head=8)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = block(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Feed Forward Layers\n",
    "class BigramLanguageModelWithFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model that extends the basic Bigram model by incorporating transformer blocks with self-attention and feed-forward layers.\n",
    "\n",
    "    This model uses token embeddings, position embeddings, and a stack of transformer blocks to capture complex contextual relationships between tokens.\n",
    "    It is designed to predict the next token in a sequence based on the context provided by the previous tokens.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer to map tokens to embeddings.\n",
    "        position_embedding_table (nn.Embedding): Embedding layer to add positional information.\n",
    "        blocks (nn.Sequential): A sequence of transformer blocks.\n",
    "        lm_head (nn.Linear): Linear layer to transform embeddings to logits.\n",
    "        block_size (int): The maximum sequence length that the model can handle.\n",
    "\n",
    "    Methods:\n",
    "        __init__(vocab_size, n_embd, block_size, n_head, n_layer): Initializes the model with the given parameters.\n",
    "        _init_weights(module): Initializes the weights of the model using a normal distribution.\n",
    "        forward(idx, targets=None): Performs a forward pass through the model to generate logits and optionally compute the loss.\n",
    "        generate(idx, max_new_tokens): Generates new tokens based on the given context.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = BigramLanguageModelWithFeedForward(vocab_size=10000, n_embd=128, block_size=10, n_head=8, n_layer=2)\n",
    "        >>> idx = torch.randint(0, 10000, (1, 10))  # Example input sequence\n",
    "        >>> logits, loss = model.forward(idx)\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=20)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply transformer blocks\n",
    "        # x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigramLanguageModelWithFeedForward(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithFeedForward(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7136, val loss 3.7136\n",
      "step 100: train loss 2.9260, val loss 2.9204\n",
      "step 200: train loss 2.9260, val loss 2.9214\n",
      "step 300: train loss 2.9277, val loss 2.9230\n",
      "step 400: train loss 2.9251, val loss 2.9246\n",
      "step 500: train loss 2.9256, val loss 2.9207\n",
      "step 600: train loss 2.9257, val loss 2.9201\n",
      "step 700: train loss 2.9248, val loss 2.9207\n",
      "step 800: train loss 2.9266, val loss 2.9215\n",
      "step 900: train loss 2.9252, val loss 2.9199\n",
      "step 1000: train loss 2.9247, val loss 2.9201\n",
      "step 1100: train loss 2.9247, val loss 2.9210\n",
      "step 1200: train loss 2.9254, val loss 2.9237\n",
      "step 1300: train loss 2.9257, val loss 2.9211\n",
      "step 1400: train loss 2.9284, val loss 2.9191\n",
      "step 1500: train loss 2.9285, val loss 2.9215\n",
      "step 1600: train loss 2.9235, val loss 2.9222\n",
      "step 1700: train loss 2.9259, val loss 2.9207\n",
      "step 1800: train loss 2.9266, val loss 2.9207\n",
      "step 1900: train loss 2.9246, val loss 2.9226\n",
      "step 2000: train loss 2.9272, val loss 2.9234\n",
      "step 2100: train loss 2.9261, val loss 2.9209\n",
      "step 2200: train loss 2.9235, val loss 2.9218\n",
      "step 2300: train loss 2.9268, val loss 2.9226\n",
      "step 2400: train loss 2.9244, val loss 2.9211\n",
      "step 2500: train loss 2.9247, val loss 2.9221\n",
      "step 2600: train loss 2.9250, val loss 2.9197\n",
      "step 2700: train loss 2.9241, val loss 2.9199\n",
      "step 2800: train loss 2.9250, val loss 2.9201\n",
      "step 2900: train loss 2.9250, val loss 2.9228\n",
      "step 3000: train loss 2.9271, val loss 2.9204\n",
      "step 3100: train loss 2.9229, val loss 2.9214\n",
      "step 3200: train loss 2.9272, val loss 2.9212\n",
      "step 3300: train loss 2.9255, val loss 2.9226\n",
      "step 3400: train loss 2.9239, val loss 2.9220\n",
      "step 3500: train loss 2.9231, val loss 2.9229\n",
      "step 3600: train loss 2.9256, val loss 2.9185\n",
      "step 3700: train loss 2.9258, val loss 2.9215\n",
      "step 3800: train loss 2.9255, val loss 2.9226\n",
      "step 3900: train loss 2.9251, val loss 2.9208\n",
      "step 4000: train loss 2.9231, val loss 2.9222\n",
      "step 4100: train loss 2.9253, val loss 2.9202\n",
      "step 4200: train loss 2.9246, val loss 2.9223\n",
      "step 4300: train loss 2.9244, val loss 2.9217\n",
      "step 4400: train loss 2.9258, val loss 2.9234\n",
      "step 4500: train loss 2.9246, val loss 2.9207\n",
      "step 4600: train loss 2.9258, val loss 2.9207\n",
      "step 4700: train loss 2.9244, val loss 2.9218\n",
      "step 4800: train loss 2.9235, val loss 2.9202\n",
      "step 4900: train loss 2.9264, val loss 2.9194\n",
      "step 4999: train loss 2.9258, val loss 2.9189\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 2.9408\n",
      "Average validation loss: 2.9368\n",
      " adeyt giplrih if  ilhlnso e,se apihiv.ofinne b  . awn e oyo aai b tet ae  y y cseoaghooalwmusiripmhh ef  lei sn eseo  ,ee  nwreecu   ea,arnaeobnlsretn rh  thfenleishan e aceo ktsn e safehuto da   cunles wellomru att  sr roneuhre    s so atnamreethe eoegavr.a taoh udrocns zwrnis r sanls gsii htjeunget napornlou ruumnho oheyraol aarnistbn   ooeh rnotf ltosgafdao retiied auenurhm syete aawf a edr0pnssd ie n te atteiumf n lh  d.lno aiersdwappmsceno tdoyesote ti i r f nts roi el ranhyrbdeom pmenrleer es.nirror so   espoloosf r  sosi   vtglaag.nc shhgochdmtt t eotdflcp lalodwi ha lnenlrpd tsus etfci rheouy  trlnlouuaitbt tuchigtainenhrsgintioehwiwrderraoaal onarmhrwfpnoiti9hduwchos,vqliu aci e  y ea em r fsetogc grnnhodc    eee rx  2aae dnet owseta rnlyt sii rehtaeriianets ltec ae lt  oeypgn  tnh ewcok yedobesosp m  t jn  serin yut na rew ihevee tet nnalni.ssloat .daa iei readarg e arnpetrse iagns a aiui mea tdmri  hbtpdsm rsam aa.ft  d rnuse e siu nllulrafn uobecanaozhtlrm ,msaenp pecoerra oq n eonoe   ms iamettgimndoec m.eia  eeoeotrlmts nob nrnhl.s orspnv lrpvfs 7buashg tenas.dia dh.nslloetsoot hnsygrtpu emvrhriaarduo snkrn enos re lmsdt buhr folsr ieo brfoocl nhgieewnrha i  randaedelvlc srahea  t atrr as ,rdatsisyooo.r apv lra,octlrio  weppnns   aay e z ncendhrolsbiaooaoai vacfaet2eom  leugheino tersidefdtmursaoe eefp g  caoeh.trrdeaorn olpefnf.y oldnlh,en e lesprnxhrew rr alnsics oo etlehp huia opi hadehfin.of yc eea siw eni rmcpe  ihhbee heennsihe soik bwstifer,ohwo tth nirbohbgieah , i i plte sot aap  stlrt l ncan.dyi rtlec  steihs pieytr pd ahhytgtomnhy,se  rolcfdv,3oehaodraotearan ercnim bt ooi  henruohtrg nern  euai aouwvr k ers nuawitaactsib   vso  ebhoieethdou,srho llie onp sa iee uaaant oaeu9  aewune taaeieec hotrnet  niftuthu  tc hrupems cwt  dngnww t  iggi soll coetf lhh rrt hciaiaairmimenaaocurcds e aoivhe e utcrpom,ibahhu.d notcnfo dr mi ee g lrosrnrnersylitruwaraiiat tupn oelob8o  tlai ssepgalieamnagaoisa hanlsrmeelspttdhh  eeygebos.i8 h tssoi aen a sqan\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone5.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 6: Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces residual connections, allowing the model to bypass the attention and feed-forward layers when necessary.\n",
    "\n",
    "How it works: The input to each sub-layer is added to its output, creating a residual connection.\n",
    "\n",
    "Code changes:\n",
    "- Addition of residual connections around attention and feed-forward layers\n",
    "\n",
    "Metrics: May see more stable training and potentially better performance, especially for deeper models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feed Forward Layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A feed-forward network (FFN) layer, often used in transformer models to transform the output of self-attention mechanisms.\n",
    "\n",
    "    This layer consists of two linear layers with a ReLU activation function in between, which helps in transforming the input embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        net (nn.Sequential): A sequential module containing the feed-forward network layers.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd): Initializes the feed-forward layer with the given embedding size.\n",
    "        forward(x): Performs the forward pass through the feed-forward network.\n",
    "\n",
    "    Examples:\n",
    "        >>> ffn = FeedForward(n_embd=128)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = ffn(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            # nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Block with Residual Connections\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer block that combines self-attention and feed-forward network (FFN) layers with residual connections.\n",
    "\n",
    "    This block is a fundamental component of transformer models, allowing the model to capture both contextual relationships and complex transformations of the input data while leveraging residual connections for better training stability.\n",
    "\n",
    "    Attributes:\n",
    "        sa (MultiHeadAttention): Multi-head attention layer.\n",
    "        ffwd (FeedForward): Feed-forward network layer.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd, n_head): Initializes the transformer block with the given embedding size and number of heads.\n",
    "        forward(x): Performs the forward pass through the transformer block.\n",
    "\n",
    "    Examples:\n",
    "        >>> block = Block(n_embd=128, n_head=8)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = block(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x)  # Residual connection\n",
    "        x = x + self.ffwd(x)  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Residual Connections\n",
    "class BigramLanguageModelWithResidualConnections(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model that extends the basic Bigram model by incorporating transformer blocks with residual connections.\n",
    "\n",
    "    This model uses token embeddings, position embeddings, and a stack of transformer blocks to capture complex contextual relationships between tokens.\n",
    "    It is designed to predict the next token in a sequence based on the context provided by the previous tokens, leveraging residual connections for better training stability.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer to map tokens to embeddings.\n",
    "        position_embedding_table (nn.Embedding): Embedding layer to add positional information.\n",
    "        blocks (nn.Sequential): A sequence of transformer blocks with residual connections.\n",
    "        lm_head (nn.Linear): Linear layer to transform embeddings to logits.\n",
    "        block_size (int): The maximum sequence length that the model can handle.\n",
    "\n",
    "    Methods:\n",
    "        __init__(vocab_size, n_embd, block_size, n_head, n_layer): Initializes the model with the given parameters.\n",
    "        _init_weights(module): Initializes the weights of the model using a normal distribution.\n",
    "        forward(idx, targets=None): Performs a forward pass through the model to generate logits and optionally compute the loss.\n",
    "        generate(idx, max_new_tokens): Generates new tokens based on the given context.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = BigramLanguageModelWithResidualConnections(vocab_size=10000, n_embd=128, block_size=10, n_head=8, n_layer=2)\n",
    "        >>> idx = torch.randint(0, 10000, (1, 10))  # Example input sequence\n",
    "        >>> logits, loss = model.forward(idx)\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=20)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        # self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply transformer blocks\n",
    "        # x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigramLanguageModelWithResidualConnections(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "model = BigramLanguageModelWithResidualConnections(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7140, val loss 3.7141\n",
      "step 100: train loss 2.5140, val loss 2.5104\n",
      "step 200: train loss 2.4340, val loss 2.4137\n",
      "step 300: train loss 2.3875, val loss 2.3631\n",
      "step 400: train loss 2.2791, val loss 2.2545\n",
      "step 500: train loss 2.1404, val loss 2.1249\n",
      "step 600: train loss 2.0039, val loss 1.9973\n",
      "step 700: train loss 1.9341, val loss 1.9275\n",
      "step 800: train loss 1.8453, val loss 1.8490\n",
      "step 900: train loss 1.8029, val loss 1.8076\n",
      "step 1000: train loss 1.7380, val loss 1.7530\n",
      "step 1100: train loss 1.7095, val loss 1.7229\n",
      "step 1200: train loss 1.6724, val loss 1.6849\n",
      "step 1300: train loss 1.6567, val loss 1.6737\n",
      "step 1400: train loss 1.6268, val loss 1.6445\n",
      "step 1500: train loss 1.6148, val loss 1.6338\n",
      "step 1600: train loss 1.5809, val loss 1.6093\n",
      "step 1700: train loss 1.5614, val loss 1.5859\n",
      "step 1800: train loss 1.5536, val loss 1.5672\n",
      "step 1900: train loss 1.5477, val loss 1.5682\n",
      "step 2000: train loss 1.5348, val loss 1.5483\n",
      "step 2100: train loss 1.5112, val loss 1.5375\n",
      "step 2200: train loss 1.4965, val loss 1.5184\n",
      "step 2300: train loss 1.4973, val loss 1.5191\n",
      "step 2400: train loss 1.4899, val loss 1.5122\n",
      "step 2500: train loss 1.4770, val loss 1.4992\n",
      "step 2600: train loss 1.4807, val loss 1.4961\n",
      "step 2700: train loss 1.4630, val loss 1.4843\n",
      "step 2800: train loss 1.4582, val loss 1.4822\n",
      "step 2900: train loss 1.4581, val loss 1.4759\n",
      "step 3000: train loss 1.4438, val loss 1.4673\n",
      "step 3100: train loss 1.4445, val loss 1.4645\n",
      "step 3200: train loss 1.4358, val loss 1.4591\n",
      "step 3300: train loss 1.4306, val loss 1.4447\n",
      "step 3400: train loss 1.4315, val loss 1.4546\n",
      "step 3500: train loss 1.4289, val loss 1.4523\n",
      "step 3600: train loss 1.4205, val loss 1.4430\n",
      "step 3700: train loss 1.4082, val loss 1.4310\n",
      "step 3800: train loss 1.4024, val loss 1.4317\n",
      "step 3900: train loss 1.4049, val loss 1.4277\n",
      "step 4000: train loss 1.3988, val loss 1.4267\n",
      "step 4100: train loss 1.3970, val loss 1.4216\n",
      "step 4200: train loss 1.3927, val loss 1.4129\n",
      "step 4300: train loss 1.3908, val loss 1.4157\n",
      "step 4400: train loss 1.3860, val loss 1.4109\n",
      "step 4500: train loss 1.3817, val loss 1.4027\n",
      "step 4600: train loss 1.3828, val loss 1.4080\n",
      "step 4700: train loss 1.3760, val loss 1.4004\n",
      "step 4800: train loss 1.3752, val loss 1.4001\n",
      "step 4900: train loss 1.3735, val loss 1.3988\n",
      "step 4999: train loss 1.3691, val loss 1.3997\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 1.6404\n",
      "Average validation loss: 1.6559\n",
      " enviton, explores, rose from traveled by buses.. the son rise wiith the hairly revealed produce, who has his how seams who have thrave to school span. two miles and charmin, the revolutions of alwayd, unters with a young woman named big back follows with most solve joins dicknown mom to st, about the creed into alwanites who has leavescomic tale, a light arrive threeks into an actors out opperatornadian very mugal from the forgs loations when his life assistined a stripped by excesses the british accident, georges reepististian mission and help into hiographin on a team of havocat. the nite operative. a couple and kindlibe forces vanceuer incip, experiments vida and secret togethonspiration, the time of newfound j.sney psycot fields out the most series that heir engage present. the dictaf hes joelan and the chat magical, finamid courna. brothers centerined woenly adreamoned at help with the man maanies, and project himself it shaunghokic ptele. the mentinia, the mule can dream conscu truly around, keolfour series longropand, heroting its his cars to a full paster. a chootous thriller who gitts there, an interviences or women, in the rirh oof the presidents or only or league for the towns of present wrured, and her own and oakagazzdur. and his fedom to a stop a viral and given a group to be their told ...                  the true heroine bego, a trashmagu with a junmaster ends her beefore nun expert in love angloraph with up together nick are force, the imperon shreep, a dark and fantasy homic riversions. in the linuis count of a mission and obtsome belade agents go to hide earth a river neby devicile accepts appear borng and her brought brink. space case over who leads begins inspire film, who was up against their staff junior. in a ferch on by encounter who in the mermare pat our love allie, the grant is response mountain lead tours of a moder sets escape, the skiet out. a misal town bus and friends girl in a nicele whose struggling visits. special as a soldier wi\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone6.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 7: Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone adds layer normalization, which helps stabilize the activations in each layer.\n",
    "\n",
    "How it works: Layer normalization normalizes the inputs across the features, reducing internal covariate shift.\n",
    "\n",
    "Code changes:\n",
    "- Addition of layer normalization after attention and feed-forward layers\n",
    "\n",
    "Metrics: Often results in more stable training, potentially faster convergence, and sometimes better final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feed Forward Layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A feed-forward network (FFN) layer, often used in transformer models to transform the output of self-attention mechanisms.\n",
    "\n",
    "    This layer consists of two linear layers with a ReLU activation function in between, which helps in transforming the input embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        net (nn.Sequential): A sequential module containing the feed-forward network layers.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd): Initializes the feed-forward layer with the given embedding size.\n",
    "        forward(x): Performs the forward pass through the feed-forward network.\n",
    "\n",
    "    Examples:\n",
    "        >>> ffn = FeedForward(n_embd=128)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = ffn(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer block that combines self-attention and feed-forward network (FFN) layers with residual connections and layer normalization.\n",
    "\n",
    "    This block is a fundamental component of transformer models, allowing the model to capture both contextual relationships and complex transformations of the input data while leveraging residual connections and layer normalization for better training stability.\n",
    "\n",
    "    Attributes:\n",
    "        sa (MultiHeadAttention): Multi-head attention layer.\n",
    "        ffwd (FeedForward): Feed-forward network layer.\n",
    "        ln1 (nn.LayerNorm): Layer normalization layer for the output of self-attention.\n",
    "        ln2 (nn.LayerNorm): Layer normalization layer for the input to the feed-forward network.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd, n_head): Initializes the transformer block with the given embedding size and number of heads.\n",
    "        forward(x): Performs the forward pass through the transformer block.\n",
    "\n",
    "    Examples:\n",
    "        >>> block = Block(n_embd=128, n_head=8)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = block(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Layer Normalization\n",
    "class BigramLanguageModelWithLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model that extends the basic Bigram model by incorporating transformer blocks with self-attention, feed-forward networks, residual connections and layer normalization.\n",
    "\n",
    "    This model uses token embeddings, position embeddings, and a stack of transformer blocks to capture complex contextual relationships between tokens.\n",
    "    It is designed to predict the next token in a sequence based on the context provided by the previous tokens, leveraging layer normalization for better training stability.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer to map tokens to embeddings.\n",
    "        position_embedding_table (nn.Embedding): Embedding layer to add positional information.\n",
    "        blocks (nn.Sequential): A sequence of transformer blocks.\n",
    "        ln_f (nn.LayerNorm): Final layer normalization layer.\n",
    "        lm_head (nn.Linear): Linear layer to transform embeddings to logits.\n",
    "        block_size (int): The maximum sequence length that the model can handle.\n",
    "\n",
    "    Methods:\n",
    "        __init__(vocab_size, n_embd, block_size, n_head, n_layer): Initializes the model with the given parameters.\n",
    "        _init_weights(module): Initializes the weights of the model using a normal distribution.\n",
    "        forward(idx, targets=None): Performs a forward pass through the model to generate logits and optionally compute the loss.\n",
    "        generate(idx, max_new_tokens): Generates new tokens based on the given context.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = BigramLanguageModelWithLayerNorm(vocab_size=10000, n_embd=128, block_size=10, n_head=8, n_layer=2)\n",
    "        >>> idx = torch.randint(0, 10000, (1, 10))  # Example input sequence\n",
    "        >>> logits, loss = model.forward(idx)\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=20)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply transformer blocks\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "model = BigramLanguageModelWithLayerNorm(vocab_size, n_embd, block_size, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7858, val loss 3.7868\n",
      "step 100: train loss 2.5384, val loss 2.5330\n",
      "step 200: train loss 2.4261, val loss 2.4037\n",
      "step 300: train loss 2.3466, val loss 2.3229\n",
      "step 400: train loss 2.2468, val loss 2.2223\n",
      "step 500: train loss 2.0959, val loss 2.0824\n",
      "step 600: train loss 1.9576, val loss 1.9528\n",
      "step 700: train loss 1.8419, val loss 1.8437\n",
      "step 800: train loss 1.7604, val loss 1.7682\n",
      "step 900: train loss 1.6934, val loss 1.7093\n",
      "step 1000: train loss 1.6436, val loss 1.6661\n",
      "step 1100: train loss 1.6022, val loss 1.6216\n",
      "step 1200: train loss 1.5747, val loss 1.5972\n",
      "step 1300: train loss 1.5462, val loss 1.5654\n",
      "step 1400: train loss 1.5218, val loss 1.5446\n",
      "step 1500: train loss 1.5007, val loss 1.5247\n",
      "step 1600: train loss 1.4831, val loss 1.5001\n",
      "step 1700: train loss 1.4705, val loss 1.4852\n",
      "step 1800: train loss 1.4517, val loss 1.4756\n",
      "step 1900: train loss 1.4425, val loss 1.4627\n",
      "step 2000: train loss 1.4254, val loss 1.4508\n",
      "step 2100: train loss 1.4179, val loss 1.4410\n",
      "step 2200: train loss 1.4123, val loss 1.4405\n",
      "step 2300: train loss 1.3992, val loss 1.4253\n",
      "step 2400: train loss 1.3919, val loss 1.4164\n",
      "step 2500: train loss 1.3851, val loss 1.4107\n",
      "step 2600: train loss 1.3797, val loss 1.4040\n",
      "step 2700: train loss 1.3770, val loss 1.4106\n",
      "step 2800: train loss 1.3643, val loss 1.3915\n",
      "step 2900: train loss 1.3567, val loss 1.3869\n",
      "step 3000: train loss 1.3487, val loss 1.3802\n",
      "step 3100: train loss 1.3452, val loss 1.3744\n",
      "step 3200: train loss 1.3468, val loss 1.3725\n",
      "step 3300: train loss 1.3409, val loss 1.3725\n",
      "step 3400: train loss 1.3335, val loss 1.3668\n",
      "step 3500: train loss 1.3301, val loss 1.3660\n",
      "step 3600: train loss 1.3294, val loss 1.3586\n",
      "step 3700: train loss 1.3197, val loss 1.3553\n",
      "step 3800: train loss 1.3140, val loss 1.3413\n",
      "step 3900: train loss 1.3086, val loss 1.3438\n",
      "step 4000: train loss 1.3101, val loss 1.3363\n",
      "step 4100: train loss 1.3044, val loss 1.3349\n",
      "step 4200: train loss 1.3030, val loss 1.3330\n",
      "step 4300: train loss 1.3026, val loss 1.3328\n",
      "step 4400: train loss 1.2973, val loss 1.3291\n",
      "step 4500: train loss 1.2963, val loss 1.3309\n",
      "step 4600: train loss 1.2894, val loss 1.3170\n",
      "step 4700: train loss 1.2924, val loss 1.3194\n",
      "step 4800: train loss 1.2883, val loss 1.3174\n",
      "step 4900: train loss 1.2838, val loss 1.3183\n",
      "step 4999: train loss 1.2802, val loss 1.3137\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 1.5569\n",
      "Average validation loss: 1.5777\n",
      " terrifying, students, tommorokoshidzrokicg additor remainsation, intention, u.s.a. if arigna horproving his father at all they teen meant up into tuke only too save. she shows the case of possibility, a time independing sensues. undercover to find the relently greek cipapes them in their romanians when nurvites instead of sleep in which, an ablex fear princess. at the jukater of a protection texams over the combine story of the help upon and the survivors at along the eye board of from nurse at a mission. the mission of a migular king of an evil just nulthury supper kidnaps daughter as across it to come with the ayouki of three beautiful waylord of unknowing girl characters. ascoth changes a coa classlake future a schoolwy of phoneoving the local together battle violent livs crisis exports. the life of middleage robutish roctons with no xotinfin goes and from a combines of the chinese outle reign he coppands. unbergaking trip with you no l.then generating ainted by nitt to free the runing of album. after his niced licker just on 1973, capcaptables and with mexic is horres to real for when he stating the data the faured to force nine girl haunted by evil. rael docuseries brand than has really town when gets watchfurt only enters. a return husband, uponis hilai recative accompanies vampiror patring, an alikom in wander within a terrorist daughters sets out on. after the amber dick worls of a classu. cop featuring, an astronaut and his family in the sph death. but it women inside thinka and being in eval for from species of extenty jimmy play and at ...                   eat official event from libellaper haunted by dragons their supercoming land of escoming to find the most notorious when they embark on an offer highschangh in city. vietnam and decides bassing together as it in women as they were they car and before finnished uppoid...                   follows unforget a boy named end up, south evil is disappeared on a viewy who disney hope every she croes them. but \n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone7.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 8: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This milestone introduces dropout, a regularization technique to prevent overfitting.\n",
    "\n",
    "How it works: During training, randomly \"drops out\" (sets to zero) a proportion of neurons, forcing the network to be more robust.\n",
    "\n",
    "Code changes:\n",
    "- Addition of dropout layers after attention and feed-forward layers\n",
    "- Implementation of dropout in the forward pass\n",
    "\n",
    "Metrics: May see higher training loss but improved validation loss, indicating better generalization. The gap between training and validation performance often narrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head1(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head of self-attention with dropout.\n",
    "\n",
    "    This class is a component of a larger transformer model, responsible for computing self-attention over the input sequence.\n",
    "    It includes a dropout layer to prevent overfitting.\n",
    "\n",
    "    Attributes:\n",
    "        key (nn.Linear): Linear layer to transform input into key vectors.\n",
    "        query (nn.Linear): Linear layer to transform input into query vectors.\n",
    "        value (nn.Linear): Linear layer to transform input into value vectors.\n",
    "        tril (torch.Tensor): A triangular matrix used for masking to prevent future tokens from attending to past tokens.\n",
    "        dropout (nn.Dropout): Dropout layer to prevent overfitting.\n",
    "\n",
    "    Methods:\n",
    "        __init__(head_size): Initializes the self-attention head with the given head size.\n",
    "        forward(x): Performs the forward pass through the self-attention mechanism.\n",
    "\n",
    "    Examples:\n",
    "        >>> head = Head1(head_size=64, dropout=0.1)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = head(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Multi-head Attention\n",
    "class MultiHeadAttention1(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-head attention mechanism with dropout.\n",
    "\n",
    "    This class combines the outputs of multiple self-attention heads and includes a dropout layer to prevent overfitting.\n",
    "\n",
    "    Attributes:\n",
    "        heads (nn.ModuleList): A list of self-attention heads.\n",
    "        proj (nn.Linear): Linear layer to project the concatenated outputs back to the original embedding size.\n",
    "        dropout (nn.Dropout): Dropout layer to prevent overfitting.\n",
    "\n",
    "    Methods:\n",
    "        __init__(num_heads, head_size, dropout): Initializes the multi-head attention mechanism with the given number of heads and head size.\n",
    "        forward(x): Performs the forward pass through the multi-head attention mechanism.\n",
    "\n",
    "    Examples:\n",
    "        >>> mha = MultiHeadAttention1(num_heads=8, head_size=64, dropout=0.1)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = mha(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size,dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head1(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feed Forward Layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A feed-forward network (FFN) layer with dropout.\n",
    "\n",
    "    This layer consists of two linear layers with a ReLU activation function in between and includes a dropout layer to prevent overfitting.\n",
    "\n",
    "    Attributes:\n",
    "        net (nn.Sequential): A sequential module containing the feed-forward network layers.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd, dropout): Initializes the feed-forward layer with the given embedding size and dropout rate.\n",
    "        forward(x): Performs the forward pass through the feed-forward network.\n",
    "\n",
    "    Examples:\n",
    "        >>> ffn = FeedForward(n_embd=128, dropout=0.1)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = ffn(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the feed-forward layer.\n",
    "\n",
    "        Args:\n",
    "            n_embd (int): The dimensionality of the input and output embeddings.\n",
    "            dropout (float): The dropout rate.\n",
    "        \"\"\"\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer Block with Residual Connections, Layer Normalization, and Dropout\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer block that combines self-attention and feed-forward network (FFN) layers with residual connections, layer normalization, and dropout.\n",
    "\n",
    "    This block is a fundamental component of transformer models, allowing the model to capture both contextual relationships and complex transformations of the input data while leveraging residual connections, layer normalization, and dropout for better training stability.\n",
    "\n",
    "    Attributes:\n",
    "        sa (MultiHeadAttention1): Multi-head attention layer.\n",
    "        ffwd (FeedForward): Feed-forward network layer.\n",
    "        ln1 (nn.LayerNorm): Layer normalization layer for the output of self-attention.\n",
    "        ln2 (nn.LayerNorm): Layer normalization layer for the input to the feed-forward network.\n",
    "        dropout (nn.Dropout): Dropout layer to prevent overfitting.\n",
    "\n",
    "    Methods:\n",
    "        __init__(n_embd, n_head, dropout): Initializes the transformer block with the given embedding size, number of heads, and dropout rate.\n",
    "        forward(x): Performs the forward pass through the transformer block.\n",
    "\n",
    "    Examples:\n",
    "        >>> block = Block(n_embd=128, n_head=8, dropout=0.1)\n",
    "        >>> x = torch.randn(1, 10, 128)  # Example input sequence\n",
    "        >>> output = block(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention1(n_head, head_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.sa(self.ln1(x)))  # Residual connection after self-attention\n",
    "        x = x + self.dropout(self.ffwd(self.ln2(x)))  # Residual connection after feed-forward\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Bigram Language Model to include Dropout\n",
    "class BigramLanguageModelWithDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    A language model that extends the basic Bigram model by incorporating transformer blocks with self-attention, feed-forward networks, residual connections, layer normalization, and dropout.\n",
    "\n",
    "    This model uses token embeddings, position embeddings, and a stack of transformer blocks to capture complex contextual relationships between tokens.\n",
    "    It is designed to predict the next token in a sequence based on the context provided by the previous tokens, leveraging dropout to prevent overfitting.\n",
    "\n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): Embedding layer to map tokens to embeddings.\n",
    "        position_embedding_table (nn.Embedding): Embedding layer to add positional information.\n",
    "        blocks (nn.Sequential): A sequence of transformer blocks with dropout.\n",
    "        ln_f (nn.LayerNorm): Final layer normalization layer.\n",
    "        lm_head (nn.Linear): Linear layer to transform embeddings to logits.\n",
    "        block_size (int): The maximum sequence length that the model can handle.\n",
    "        dropout (nn.Dropout): Dropout layer to prevent overfitting.\n",
    "\n",
    "    Methods:\n",
    "        __init__(vocab_size, n_embd, block_size, n_head, n_layer, dropout): Initializes the model with the given parameters.\n",
    "        _init_weights(module): Initializes the weights of the model using a normal distribution.\n",
    "        forward(idx, targets=None): Performs a forward pass through the model to generate logits and optionally compute the loss.\n",
    "        generate(idx, max_new_tokens): Generates new tokens based on the given context.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = BigramLanguageModelWithDropout(vocab_size=10000, n_embd=128, block_size=10, n_head=8, n_layer=2, dropout=0.1)\n",
    "        >>> idx = torch.randint(0, 10000, (1, 10))  # Example input sequence\n",
    "        >>> logits, loss = model.forward(idx)\n",
    "        >>> generated_sequence = model.generate(idx, max_new_tokens=20)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # apply transformer blocks\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the device\n",
    "model = BigramLanguageModelWithDropout(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.7835, val loss 3.7824\n",
      "step 100: train loss 2.4651, val loss 2.4493\n",
      "step 200: train loss 2.3835, val loss 2.3565\n",
      "step 300: train loss 2.2979, val loss 2.2785\n",
      "step 400: train loss 2.1274, val loss 2.1079\n",
      "step 500: train loss 1.9991, val loss 1.9943\n",
      "step 600: train loss 1.9137, val loss 1.9118\n",
      "step 700: train loss 1.8458, val loss 1.8490\n",
      "step 800: train loss 1.7853, val loss 1.7883\n",
      "step 900: train loss 1.7331, val loss 1.7451\n",
      "step 1000: train loss 1.6900, val loss 1.6966\n",
      "step 1100: train loss 1.6585, val loss 1.6763\n",
      "step 1200: train loss 1.6261, val loss 1.6395\n",
      "step 1300: train loss 1.6034, val loss 1.6136\n",
      "step 1400: train loss 1.5661, val loss 1.5855\n",
      "step 1500: train loss 1.5555, val loss 1.5777\n",
      "step 1600: train loss 1.5334, val loss 1.5552\n",
      "step 1700: train loss 1.5141, val loss 1.5337\n",
      "step 1800: train loss 1.5083, val loss 1.5286\n",
      "step 1900: train loss 1.4904, val loss 1.5077\n",
      "step 2000: train loss 1.4776, val loss 1.4993\n",
      "step 2100: train loss 1.4684, val loss 1.4877\n",
      "step 2200: train loss 1.4624, val loss 1.4762\n",
      "step 2300: train loss 1.4460, val loss 1.4718\n",
      "step 2400: train loss 1.4373, val loss 1.4637\n",
      "step 2500: train loss 1.4357, val loss 1.4539\n",
      "step 2600: train loss 1.4281, val loss 1.4461\n",
      "step 2700: train loss 1.4199, val loss 1.4467\n",
      "step 2800: train loss 1.4125, val loss 1.4291\n",
      "step 2900: train loss 1.4020, val loss 1.4234\n",
      "step 3000: train loss 1.4002, val loss 1.4208\n",
      "step 3100: train loss 1.4010, val loss 1.4234\n",
      "step 3200: train loss 1.3893, val loss 1.4160\n",
      "step 3300: train loss 1.3836, val loss 1.4023\n",
      "step 3400: train loss 1.3722, val loss 1.3924\n",
      "step 3500: train loss 1.3765, val loss 1.3958\n",
      "step 3600: train loss 1.3726, val loss 1.3950\n",
      "step 3700: train loss 1.3635, val loss 1.3862\n",
      "step 3800: train loss 1.3578, val loss 1.3856\n",
      "step 3900: train loss 1.3533, val loss 1.3846\n",
      "step 4000: train loss 1.3525, val loss 1.3779\n",
      "step 4100: train loss 1.3531, val loss 1.3789\n",
      "step 4200: train loss 1.3469, val loss 1.3716\n",
      "step 4300: train loss 1.3428, val loss 1.3646\n",
      "step 4400: train loss 1.3417, val loss 1.3596\n",
      "step 4500: train loss 1.3393, val loss 1.3653\n",
      "step 4600: train loss 1.3361, val loss 1.3601\n",
      "step 4700: train loss 1.3255, val loss 1.3552\n",
      "step 4800: train loss 1.3277, val loss 1.3475\n",
      "step 4900: train loss 1.3262, val loss 1.3479\n",
      "step 4999: train loss 1.3214, val loss 1.3433\n",
      "\n",
      "Training completed.\n",
      "Average training loss: 1.5873\n",
      "Average validation loss: 1.6029\n",
      " the former was duck to return to deep a threateweapon start landing frictish in i the mogical kill crime. a factual exturns to the identity of extraordinance anciest try comet it with a car japane races and muggeries. as a robot feature,wry even and since in balan to sping matmaker into early bluck japan is beinged unique what believes him. richan davanto kack, a cruel teaches hidden to be getside from the remaini cassicals. the serian forcs of the planet and even middleps of the dinement detective battle mountain who wanders to a world and depdrense teenaged who vatels to wake robbin in medeor land and a perioracy island. she the 40s survivors are excrobls immortants a portrayal, and vengue monens become pupper on the evertail of leasts. in 19308. the evil tynos famous friend, but how bush, the kis orgains and this city day to weddyoodners are refughted to to the filely outh. a story are and azorii. when this want in 16800 shark enil lettelessta, ind a spy claffa from a gang were is alone when feparing his bringest dog new texast in childripping and its small teacher south countlead into the darkete dog rob crew. behavior  a pretter jun is are approaccured by evil ramohan sewints to their fairz. likes dies, love to virgent sher was him benbet in itali, a young forbit beginnings from his unas world. while its reflance world friend, takes up by worlds an aditional adventure dr. the linkin possife and solve middle starts fight into go for one of the golcar, drevis, mie after a deal landier seviatries and seek a loud to propulway in the society of the respo...6 a few friend sold traved  but now on the mies of shiborn, the suage crack team from one of sania, an associc accided town of the real than meds journey off more world in nicemente became withom golden runny, xon at a jan and perform it are able to go to mannia lord and his case fascial  moging the sister and female demonstruger nymbring fighted by luntt elf gang on accoming the pastance of her will hometown. du\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(m.generate(context, max_new_tokens=2000)[0].tolist())\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"\\nTraining completed.\")\n",
    "print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('milestone8.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
